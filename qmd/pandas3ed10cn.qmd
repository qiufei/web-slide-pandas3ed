---
title: "Python æ•°æ®åˆ†æ"
subtitle: "ç¬¬åç« "
---

## æœ¬ç« å†…å®¹ ğŸ¤”

æœ¬èŠ‚é‡ç‚¹ä»‹ç»**æ•°æ®èšåˆä¸åˆ†ç»„æ“ä½œ**ï¼Œè¿™æ˜¯æ•°æ®åˆ†æçš„å…³é”®éƒ¨åˆ†ã€‚æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¯¹æ¯ä¸ªç»„åº”ç”¨å‡½æ•°â€”â€”è¿™æ˜¯è®¸å¤šæ•°æ®å·¥ä½œæµç¨‹ä¸­çš„åŸºæœ¬æ­¥éª¤ã€‚

## æ ¸å¿ƒæ“ä½œ âš™ï¸

æˆ‘ä»¬å°†å­¦ä¹ ï¼š

-   å°† pandas å¯¹è±¡æ‹†åˆ†ä¸ºç»„ã€‚
-   è®¡ç®—ç»„æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ï¼ˆè®¡æ•°ã€å¹³å‡å€¼ç­‰ï¼‰ã€‚
-   åœ¨ç»„å†…åº”ç”¨è½¬æ¢ã€‚
-   è®¡ç®—æ•°æ®é€è§†è¡¨å’Œäº¤å‰è¡¨ã€‚
-   æ‰§è¡Œåˆ†ä½æ•°åˆ†æã€‚
-   ä½¿ç”¨ `transform` å’Œ `apply`ã€‚

## ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼ŸğŸ“ˆ

åˆ†ç»„å’Œèšåˆæ•°æ®æœ‰åŠ©äºæˆ‘ä»¬ï¼š

-   é€šè¿‡æ±‡æ€»å¤§å‹æ•°æ®é›†**è·å¾—è§è§£**ã€‚
-   **æ¯”è¾ƒ**ä¸åŒçš„ç»„ã€‚
-   **å‡†å¤‡æ•°æ®**ä»¥è¿›è¡Œåˆ†ææˆ–å¯è§†åŒ–ã€‚
-   **å‘ç°æ¨¡å¼**ã€‚

# å¦‚ä½•ç†è§£åˆ†ç»„æ“ä½œ

## â€œæ‹†åˆ†-åº”ç”¨-åˆå¹¶â€èŒƒå¼ ğŸ’¡

Hadley Wickham çš„ **â€œæ‹†åˆ†-åº”ç”¨-åˆå¹¶â€** (split-apply-combine) èŒƒå¼æ˜¯ç†è§£æ•°æ®å¤„ç†çš„å¼ºå¤§æ–¹å¼ã€‚

1.  **æ‹†åˆ† (Split)ï¼š** æ ¹æ®â€œé”®â€å°†æ•°æ®åˆ†æˆç»„ã€‚
2.  **åº”ç”¨ (Apply)ï¼š** å¯¹æ¯ä¸ªç»„åº”ç”¨ä¸€ä¸ªå‡½æ•°ã€‚
3.  **åˆå¹¶ (Combine)ï¼š** å°†ç»“æœåˆå¹¶ä¸ºæœ€ç»ˆè¾“å‡ºã€‚

## â€œæ‹†åˆ†-åº”ç”¨-åˆå¹¶â€å¯è§†åŒ– ğŸ“Š

```{mermaid}
graph LR
    A[æ•°æ®] --> B(æŒ‰é”®æ‹†åˆ†)
    B --> C1[ç»„ 1]
    B --> C2[ç»„ 2]
    B --> C3[ç»„ 3]
    C1 --> D1(åº”ç”¨å‡½æ•°)
    C2 --> D2(åº”ç”¨å‡½æ•°)
    C3 --> D3(åº”ç”¨å‡½æ•°)
    D1 --> E[åˆå¹¶ç»“æœ]
    D2 --> E
    D3 --> E
```

## åˆ†ç»„é”® ğŸ”‘

é”®å¯ä»¥æ˜¯ï¼š

-   åˆ—è¡¨æˆ–æ•°ç»„ã€‚
-   DataFrame åˆ—åã€‚
-   å°†å€¼æ˜ å°„åˆ°ç»„åçš„å­—å…¸æˆ– Seriesã€‚
-   åº”ç”¨äºç´¢å¼•çš„å‡½æ•°ã€‚

è¿™äº›æœ¬è´¨ä¸Šéƒ½æ˜¯ç”¨äºæ‹†åˆ†å¯¹è±¡çš„æ•°ç»„å€¼çš„å¿«æ·æ–¹å¼ã€‚

## ç¤ºä¾‹ï¼šç®€å•åˆ†ç»„èšåˆ

![Split-Apply-Combine](https://pandas.pydata.org/docs/_images/06_groupby.svg)

-   **é”®å’Œæ•°æ®**ï¼šå¸¦æœ‰â€œKeyâ€å’Œâ€œDataâ€åˆ—çš„è¡¨æ ¼ã€‚
-   **æ‹†åˆ†**ï¼šæ•°æ®æŒ‰â€œKeyâ€æ‹†åˆ†ä¸ºç»„ï¼ˆAã€Bã€Cï¼‰ã€‚
-   **åº”ç”¨**ï¼š`Sum` å‡½æ•°åº”ç”¨äºæ¯ä¸ªç»„çš„â€œDataâ€ã€‚
-   **åˆå¹¶**ï¼šæ€»å’Œåˆå¹¶åˆ°ä¸€ä¸ªæ–°è¡¨ä¸­ã€‚

# Pandas å®æˆ˜ï¼šåˆ›å»º DataFrame

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç¤ºä¾‹ DataFrameï¼š

```{python}
import numpy as np  # å¯¼å…¥ NumPy åº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—
import pandas as pd  # å¯¼å…¥ pandas åº“ï¼Œç”¨äºæ•°æ®åˆ†æ

# åˆ›å»ºä¸€ä¸ª DataFrame
df = pd.DataFrame({
    "key1": ["a", "a", "b", "b", "a"],  # key1 åˆ—ï¼ŒåŒ…å«å­—ç¬¦ä¸²
    "key2": [1, 2, 1, 2, 1],  # key2 åˆ—ï¼ŒåŒ…å«æ•´æ•°
    "data1": np.random.randn(5),  # data1 åˆ—ï¼ŒåŒ…å« 5 ä¸ªéšæœºæ•°
    "data2": np.random.randn(5)  # data2 åˆ—ï¼ŒåŒ…å« 5 ä¸ªéšæœºæ•°
})
df  # æ˜¾ç¤º DataFrame
```

æ­¤ DataFrame æœ‰ä¸¤ä¸ªé”®åˆ—ï¼ˆ`key1`ã€`key2`ï¼‰å’Œä¸¤ä¸ªæ•°æ®åˆ—ï¼ˆ`data1`ã€`data2`ï¼‰ã€‚

# åŸºæœ¬ GroupBy æ“ä½œ

## æŒ‰å•åˆ—åˆ†ç»„

è®¡ç®— `key1` ä¸­æ¯ä¸ªç»„çš„ `data1` çš„å¹³å‡å€¼ï¼š

```{python}
grouped = df["data1"].groupby(df["key1"])  # æŒ‰ key1 åˆ†ç»„ data1
grouped.mean()  # è®¡ç®—æ¯ä¸ªç»„çš„å¹³å‡å€¼
```

-   `df["data1"]`ï¼šé€‰æ‹© `data1` åˆ—ã€‚
-   `.groupby(df["key1"])`ï¼šæŒ‰ `key1` åˆ—åˆ†ç»„ã€‚
-   `grouped`ï¼šä¸€ä¸ª `GroupBy` å¯¹è±¡ï¼Œå­˜å‚¨åˆ†ç»„ä¿¡æ¯ã€‚
-   `.mean()`ï¼šè®¡ç®—æ¯ä¸ªç»„çš„å¹³å‡å€¼ã€‚

## æŒ‰å¤šåˆ—åˆ†ç»„

æŒ‰å¤šåˆ—åˆ†ç»„ï¼ˆåˆ†å±‚ç´¢å¼•ï¼‰ï¼š

```{python}
grouped = df.groupby(["key1", "key2"])  # æŒ‰ key1 å’Œ key2 åˆ†ç»„
grouped.agg({
    "data1": ["mean", "std"],  # å¯¹ data1 è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
    "data2": ["mean", "std"]   # å¯¹ data2 è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
})
```

æ•°æ®æŒ‰ `key1` å’Œ `key2` çš„ç»„åˆè¿›è¡Œåˆ†ç»„ã€‚

## å±•å¼€ (Unstacking)

`unstack()` é‡å¡‘ç»“æœï¼š

```{python}
means = df.groupby(["key1", "key2"])["data1"].mean()  # æŒ‰ key1 å’Œ key2 åˆ†ç»„ï¼Œè®¡ç®— data1 çš„å¹³å‡å€¼
means.unstack()  # å°†ç»“æœå±•å¼€ï¼Œä½¿ key2 æˆä¸ºåˆ—
```

## ä½¿ç”¨ Series å’Œæ•°ç»„åˆ†ç»„

é”®å¯ä»¥æ˜¯å¤–éƒ¨ Series æˆ–æ•°ç»„ï¼š

```{python}
states = np.array(["OH", "CA", "CA", "OH", "OH", "CA", "OH"])  # å¤–éƒ¨æ•°ç»„ states
years = np.array([2005, 2005, 2006, 2005, 2006, 2005, 2006])  # å¤–éƒ¨æ•°ç»„ years

# åˆ›å»ºæ–°çš„ df ä»¥åŒ¹é… states å’Œ years çš„é•¿åº¦
df_ext = pd.DataFrame({
    "key1": ["a", "a", "b", "b", "a", "b", "a"],
    "key2": [1, 2, 1, 2, 1, 2, 1],
    "data1": np.random.randn(7),
    "data2": np.random.randn(7)
})

result = df_ext["data1"].groupby([states, years]).mean()  # ä½¿ç”¨å¤–éƒ¨æ•°ç»„åˆ†ç»„
print("\nåˆ†ç»„ç»“æœ:")
print(result)

print("\nå±•å¼€ç»“æœ:")
print(result.unstack())
```

è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‰å¤–éƒ¨æ•°ç»„ `states` å’Œ `years` å¯¹ `data1` è¿›è¡Œåˆ†ç»„ã€‚

## ç›´æ¥ä½¿ç”¨åˆ—ååˆ†ç»„

å¦‚æœåˆ†ç»„ä¿¡æ¯åœ¨ DataFrame ä¸­ï¼Œç›´æ¥ä½¿ç”¨åˆ—åï¼š

```{python}
df.groupby("key1").mean()  # æŒ‰ key1 åˆ†ç»„ï¼Œè®¡ç®—æ‰€æœ‰æ•°å€¼åˆ—çš„å¹³å‡å€¼
```
éæ•°å€¼åˆ— `key1` ä¼šè¢«è‡ªåŠ¨æ’é™¤ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ª**å¹²æ‰°åˆ—**

```{python}
df.groupby(["key1", "key2"]).mean()  # æŒ‰ key1 å’Œ key2 åˆ†ç»„ï¼Œè®¡ç®—æ‰€æœ‰æ•°å€¼åˆ—çš„å¹³å‡å€¼
```

## ç»„å¤§å°

`size()` æ˜¾ç¤ºæ¯ä¸ªç»„ä¸­çš„æ•°æ®ç‚¹æ•°é‡ï¼š

```{python}
df.groupby(["key1", "key2"]).size()  # æŒ‰ key1 å’Œ key2 åˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªç»„çš„å¤§å°
```

## å¤„ç†åˆ†ç»„é”®ä¸­çš„ç¼ºå¤±å€¼

é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ†ç»„é”®ä¸­çš„ç¼ºå¤±å€¼ä¼šè¢«æ’é™¤ã€‚ä½¿ç”¨ `dropna=False` åŒ…å«å®ƒä»¬ï¼š

```{python}
df.groupby("key1", dropna=False).size()  # æŒ‰ key1 åˆ†ç»„ï¼ŒåŒ…æ‹¬ç¼ºå¤±å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
```

# è¿­ä»£ç»„

## ä½¿ç”¨å•é”®è¿­ä»£

`GroupBy` æ”¯æŒè¿­ä»£ï¼Œç”Ÿæˆç»„åå’Œæ•°æ®å—ï¼š

```{python}
for name, group in df.groupby("key1"):  # æŒ‰ key1 è¿­ä»£ç»„
    print(f"ç»„å: {name}")  # æ‰“å°ç»„å
    print(group)  # æ‰“å°ç»„æ•°æ®
```

## ä½¿ç”¨å¤šé”®è¿­ä»£

ä½¿ç”¨å¤šä¸ªé”®æ—¶ï¼Œç»„åæ˜¯ä¸€ä¸ªå…ƒç»„ï¼š

```{python}
for (k1, k2), group in df.groupby(["key1", "key2"]):  # æŒ‰ key1 å’Œ key2 è¿­ä»£ç»„
    print(f"ç»„é”®: {(k1, k2)}")  # æ‰“å°ç»„é”®
    print(group)  # æ‰“å°ç»„æ•°æ®
```

## åˆ›å»ºæ•°æ®å—å­—å…¸

åˆ›å»ºç»„æ•°æ®çš„å­—å…¸ï¼š

```{python}
pieces = dict(list(df.groupby("key1")))  # å°†æŒ‰ key1 åˆ†ç»„çš„ç»“æœè½¬æ¢ä¸ºå­—å…¸
pieces["b"]  # è·å– key1 ä¸º "b" çš„ç»„
```

# æŒ‰åˆ—åˆ†ç»„ (axis=1)

ä½¿ç”¨ `axis="columns"` æŒ‰åˆ—åˆ†ç»„ï¼š

```{python}
# åˆ›å»ºä¸€ä¸ªæ˜ å°„å­—å…¸, å°†åˆ—åæ˜ å°„åˆ°æ–°çš„åˆ†ç»„åç§°
grouped = df.groupby({
    "key1": "key",
    "key2": "key",
    "data1": "data",
    "data2": "data"
}, axis="columns")

for group_key, group_values in grouped:  # è¿­ä»£åˆ—ç»„
    print(group_key)  # æ‰“å°åˆ—ç»„é”®
    print(group_values)  # æ‰“å°åˆ—ç»„æ•°æ®
```

# é€‰æ‹©è¦èšåˆçš„åˆ—

## é€‰æ‹©å•åˆ— (SeriesGroupBy)

å¯¹ `GroupBy` å¯¹è±¡è¿›è¡Œç´¢å¼•ä»¥èšåˆç‰¹å®šåˆ—ï¼š

```{python}
df.groupby(["key1", "key2"])["data2"].mean()  # æŒ‰ key1 å’Œ key2 åˆ†ç»„ï¼Œè®¡ç®— data2 çš„å¹³å‡å€¼
```

è¿™æ˜¯ `df["data2"].groupby([df["key1"], df["key2"]]).mean()` çš„ç®€å†™ã€‚

## é€‰æ‹©å¤šåˆ— (DataFrameGroupBy)

```{python}
df.groupby(["key1", "key2"])[["data2"]].mean()  # æŒ‰ key1 å’Œ key2 åˆ†ç»„ï¼Œè®¡ç®— data2 çš„å¹³å‡å€¼
```

è¿™ç­‰åŒäº `df[["data2"]].groupby([df["key1"], df["key2"]]).mean()`ã€‚

# ä½¿ç”¨å­—å…¸å’Œ Series åˆ†ç»„

ä½¿ç”¨å­—å…¸æˆ– Series è¿›è¡Œåˆ†ç»„ï¼š

```{python}
people = pd.DataFrame(np.random.standard_normal((5, 5)),  # åˆ›å»ºä¸€ä¸ª 5x5 çš„éšæœºæ•° DataFrame
                   columns=["a", "b", "c", "d", "e"],  # åˆ—å
                   index=["Joe", "Steve", "Wanda", "Jill", "Trey"])  # è¡Œå
people.iloc[2:3, [1, 2]] = np.nan  # å°†ä¸€äº›å€¼è®¾ç½®ä¸º NaN

mapping = {"a": "red", "b": "red", "c": "blue",  # åˆ›å»ºä¸€ä¸ªåˆ—æ˜ å°„å­—å…¸
           "d": "blue", "e": "red", "f" : "orange"}

by_column = people.groupby(mapping, axis="columns")  # ä½¿ç”¨æ˜ å°„å­—å…¸æŒ‰åˆ—åˆ†ç»„
by_column.sum()  # è®¡ç®—æ¯ä¸ªåˆ—ç»„çš„æ€»å’Œ
```

`mapping` æŒ‡å®šåˆ—åˆ†ç»„ã€‚

# ä½¿ç”¨å‡½æ•°åˆ†ç»„

ä½¿ç”¨å‡½æ•°å®šä¹‰ç»„æ˜ å°„ï¼ˆæ¯ä¸ªç´¢å¼•å€¼è°ƒç”¨ä¸€æ¬¡ï¼‰ï¼š

```{python}
people.groupby(len).sum()  # æŒ‰è¡Œåçš„é•¿åº¦åˆ†ç»„ï¼Œè®¡ç®—æ€»å’Œ
```

# æŒ‰ç´¢å¼•çº§åˆ«åˆ†ç»„

æŒ‰åˆ†å±‚ç´¢å¼•çš„çº§åˆ«åˆ†ç»„ï¼š

```{python}
columns = pd.MultiIndex.from_arrays([["US", "US", "US", "JP", "JP"],  # åˆ›å»ºä¸€ä¸ªå¤šçº§åˆ—ç´¢å¼•
                                    [1, 3, 5, 1, 3]],
                                   names=["cty", "tenor"])
hier_df = pd.DataFrame(np.random.standard_normal((4, 5)), columns=columns)  # åˆ›å»ºä¸€ä¸ªå…·æœ‰å¤šçº§åˆ—ç´¢å¼•çš„ DataFrame
hier_df.groupby(level="cty", axis="columns").count()  # æŒ‰åˆ—çš„ "cty" çº§åˆ«åˆ†ç»„ï¼Œè®¡æ•°
```

# æ•°æ®èšåˆ

## ä¼˜åŒ–çš„èšåˆæ–¹æ³•

èšåˆå°†æ•°ç»„è½¬æ¢ä¸ºæ ‡é‡ã€‚ä¼˜åŒ–æ–¹æ³•ï¼š

-   `count`ã€`sum`ã€`mean`ã€`median`ã€`std`ã€`var`
-   `min`ã€`max`ã€`prod`ã€`first`ã€`last`
-   `any`ã€`all`ã€`cummin`ã€`cummax`ã€`cumsum`ã€`cumprod`
-   `nth`ã€`ohlc`ã€`quantile`ã€`rank`ã€`size`

## ä½¿ç”¨è‡ªå®šä¹‰èšåˆå‡½æ•°

ä½¿ç”¨ `agg` å®šä¹‰è‡ªå®šä¹‰å‡½æ•°ï¼š

```{python}
def peak_to_peak(arr):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—æ•°ç»„çš„æå·®ï¼ˆæœ€å¤§å€¼ - æœ€å°å€¼ï¼‰
    return arr.max() - arr.min()

grouped = df.groupby("key1")  # æŒ‰ key1 åˆ†ç»„
grouped.agg(peak_to_peak)  # åº”ç”¨è‡ªå®šä¹‰å‡½æ•°
```

## `describe` æ–¹æ³•

ä½¿ç”¨éèšåˆæ–¹æ³•ï¼Œå¦‚ `describe`ï¼š

```{python}
grouped.describe()  # è·å–æ¯ä¸ªç»„çš„æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯
```

# åˆ—å¼å’Œå¤šå‡½æ•°åº”ç”¨

## å•åˆ—ä¸Šçš„å¤šä¸ªå‡½æ•°

åŠ è½½å°è´¹æ•°æ®é›†ï¼š

```{python}
tips = pd.read_csv("examples/tips.csv")  # ä» CSV æ–‡ä»¶åŠ è½½æ•°æ®(éœ€ç¡®ä¿ examples æ–‡ä»¶å¤¹åŠ tips.csv æ–‡ä»¶å­˜åœ¨)
tips["tip_pct"] = tips["tip"] / tips["total_bill"]  # æ·»åŠ ä¸€ä¸ªå°è´¹ç™¾åˆ†æ¯”åˆ—
```

åº”ç”¨å¤šä¸ªå‡½æ•°ï¼š

```{python}
grouped = tips.groupby(["day", "smoker"])  # æŒ‰ day å’Œ smoker åˆ†ç»„
grouped_pct = grouped["tip_pct"]  # é€‰æ‹© tip_pct åˆ—
grouped_pct.agg(["mean", "std", peak_to_peak])  # åº”ç”¨å¤šä¸ªèšåˆå‡½æ•°
```

## è‡ªå®šä¹‰åˆ—å

æä¾›è‡ªå®šä¹‰åç§°ï¼š

```{python}
grouped_pct.agg([("average", "mean"), ("stdev", np.std)])  # ä½¿ç”¨è‡ªå®šä¹‰åˆ—å
```

## å¯¹ä¸åŒåˆ—åº”ç”¨ä¸åŒå‡½æ•°

å¯¹ä¸åŒåˆ—åº”ç”¨ä¸åŒçš„å‡½æ•°ï¼š

```{python}
functions = ["count", "mean", "max"]  # å®šä¹‰ä¸€ä¸ªå‡½æ•°åˆ—è¡¨
result = grouped[["tip_pct", "total_bill"]].agg(functions)  # å¯¹ tip_pct å’Œ total_bill åº”ç”¨å‡½æ•°åˆ—è¡¨
result
```

```{python}
grouped.agg({"tip" : np.max, "size" : "sum"})  # å¯¹ tip åˆ—åº”ç”¨ max å‡½æ•°ï¼Œå¯¹ size åˆ—åº”ç”¨ sum å‡½æ•°
```

```{python}
grouped.agg({
    "tip_pct": ["min", "max", "mean", "std"],  # å¯¹ tip_pct åº”ç”¨å¤šä¸ªå‡½æ•°
    "size": "sum"  # å¯¹ size åº”ç”¨ sum å‡½æ•°
})
```

# è¿”å›æ— è¡Œç´¢å¼•çš„èšåˆæ•°æ®

ä½¿ç”¨ `as_index=False` é˜»æ­¢ç»„é”®æˆä¸ºç´¢å¼•ï¼š

```{python}
numeric_cols = tips.select_dtypes(include=[np.number]).columns  # è·å–æ‰€æœ‰æ•°å€¼åˆ—
tips.groupby(["day", "smoker"], as_index=False)[numeric_cols].mean()  # æŒ‰ day å’Œ smoker åˆ†ç»„ï¼Œè®¡ç®—æ•°å€¼åˆ—çš„å¹³å‡å€¼ï¼Œä¸ä½¿ç”¨ç»„é”®ä½œä¸ºç´¢å¼•
```

# `apply`ï¼šé€šç”¨â€œæ‹†åˆ†-åº”ç”¨-åˆå¹¶â€

## `apply` çš„å¼ºå¤§ä¹‹å¤„ ğŸ’ª

`apply` æ˜¯æœ€é€šç”¨çš„ `GroupBy` æ–¹æ³•ã€‚æ‹†åˆ†ã€åº”ç”¨å‡½æ•°ã€è¿æ¥ã€‚

## ç¤ºä¾‹ï¼šé€‰æ‹©å‰å‡ è¡Œ

```{python}
def top(df, n=5, column="tip_pct"):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼ŒæŒ‰æŒ‡å®šåˆ—é€‰æ‹©å‰ n è¡Œ
    return df.sort_values(column, ascending=False)[:n]

tips.groupby("smoker").apply(top)  # æŒ‰ smoker åˆ†ç»„ï¼Œåº”ç”¨ top å‡½æ•°
```

## å‘ `apply` ä¼ é€’å‚æ•°

```{python}
tips.groupby(["smoker", "day"]).apply(top, n=1, column="total_bill")  # å‘ apply ä¼ é€’é¢å¤–çš„å‚æ•°
```

## åœ¨ `apply` ä¸­æŠ‘åˆ¶ç»„é”®

ä½¿ç”¨ `group_keys=False`ï¼š

```{python}
tips.groupby("smoker", group_keys=False).apply(top)  # æŠ‘åˆ¶ç»„é”®
```

# åˆ†ä½æ•°å’Œæ¡¶åˆ†æ

## å°† `cut` å’Œ `qcut` ä¸ `groupby` ç»“åˆä½¿ç”¨

å°† `cut`/`qcut` ä¸ `groupby` ç»“åˆä½¿ç”¨è¿›è¡Œæ¡¶/åˆ†ä½æ•°åˆ†æï¼š

```{python}
frame = pd.DataFrame({
    "data1": np.random.standard_normal(1000),  # åˆ›å»ºä¸€ä¸ªåŒ…å« 1000 ä¸ªéšæœºæ•°çš„ DataFrame
    "data2": np.random.standard_normal(1000)
})
quartiles = pd.cut(frame["data1"], 4)  # å°† data1 åˆ—åˆ†ä¸º 4 ä¸ªæ¡¶

def get_stats(group):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—ç»„çš„ç»Ÿè®¡ä¿¡æ¯
    return pd.DataFrame(
        {"min": group.min(), "max": group.max(),
        "count": group.count(), "mean": group.mean()}
    )

grouped = frame.groupby(quartiles)  # æŒ‰åˆ†ä½æ•°åˆ†ç»„
grouped.apply(get_stats)  # åº”ç”¨ get_stats å‡½æ•°
```
ç›¸åŒçš„ç»“æœå¯ä»¥ç”¨æ›´ç®€å•çš„æ–¹æ³•è®¡ç®—å¾—åˆ°:

```{python}
grouped.agg(["min", "max", "count", "mean"])
```
## ä½¿ç”¨ `qcut` è·å–ç›¸ç­‰å¤§å°çš„æ¡¶

```{python}
quartiles_samp = pd.qcut(frame["data1"], 4, labels=False)  # å°† data1 åˆ—åˆ†ä¸º 4 ä¸ªç›¸ç­‰å¤§å°çš„æ¡¶ï¼Œè¿”å›æ ‡ç­¾
grouped = frame.groupby(quartiles_samp)  # æŒ‰åˆ†ä½æ•°æ ‡ç­¾åˆ†ç»„
grouped.apply(get_stats)  # åº”ç”¨ get_stats å‡½æ•°
```

# ç¤ºä¾‹ï¼šä½¿ç”¨ç»„ç‰¹å®šå€¼å¡«å……ç¼ºå¤±å€¼

## ä½¿ç”¨å¹³å‡å€¼å¡«å……

```{python}
s = pd.Series(np.random.standard_normal(6))  # åˆ›å»ºä¸€ä¸ªåŒ…å« 6 ä¸ªéšæœºæ•°çš„ Series
s[::2] = np.nan  # å°†ä¸€äº›å€¼è®¾ç½®ä¸º NaN
s.fillna(s.mean())  # ä½¿ç”¨ Series çš„å¹³å‡å€¼å¡«å…… NaN
```

## ä½¿ç”¨ç»„ç‰¹å®šå¹³å‡å€¼å¡«å……

```{python}
states = ["Ohio", "New York", "Vermont", "Florida",  # åˆ›å»ºä¸€ä¸ªå·åˆ—è¡¨
          "Oregon", "Nevada", "California", "Idaho"]
group_key = ["East"] * 4 + ["West"] * 4  # åˆ›å»ºä¸€ä¸ªåˆ†ç»„é”®åˆ—è¡¨
data = pd.Series(np.random.standard_normal(8), index=states)  # åˆ›å»ºä¸€ä¸ª Seriesï¼Œç´¢å¼•ä¸ºå·
data[["Vermont", "Nevada", "Idaho"]] = np.nan  # å°†ä¸€äº›å€¼è®¾ç½®ä¸º NaN

def fill_mean(group):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ç»„çš„å¹³å‡å€¼å¡«å…… NaN
    return group.fillna(group.mean())

data.groupby(group_key).apply(fill_mean)  # æŒ‰ç»„é”®åˆ†ç»„ï¼Œåº”ç”¨ fill_mean å‡½æ•°
```

## é¢„å®šä¹‰å¡«å……å€¼

```{python}
fill_values = {"East": 0.5, "West": -1}  # åˆ›å»ºä¸€ä¸ªå¡«å……å€¼å­—å…¸
def fill_func(group):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨é¢„å®šä¹‰å€¼å¡«å…… NaN
    return group.fillna(fill_values[group.name])

data.groupby(group_key).apply(fill_func)  # æŒ‰ç»„é”®åˆ†ç»„ï¼Œåº”ç”¨ fill_func å‡½æ•°
```

# ç¤ºä¾‹ï¼šéšæœºæŠ½æ ·å’Œæ’åˆ—

## æ¨¡æ‹Ÿä¸€å‰¯ç‰Œ

```{python}
suits = ["H", "S", "C", "D"]  # èŠ±è‰²
card_val = (list(range(1, 11)) + [10] * 3) * 4  # ç‰Œå€¼
base_names = ["A"] + list(range(2, 11)) + ["J", "K", "Q"]  # ç‰Œå
cards = []
for suit in suits:
    cards.extend(str(num) + suit for num in base_names)

deck = pd.Series(card_val, index=cards)  # åˆ›å»ºä¸€å‰¯ç‰Œ
```

## æŠ½å–éšæœºä¸€æ‰‹ç‰Œ

```{python}
def draw(deck, n=5):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»ä¸€å‰¯ç‰Œä¸­æŠ½å– n å¼ ç‰Œ
    return deck.sample(n)

draw(deck)  # æŠ½å– 5 å¼ ç‰Œ
```

## ä»æ¯ä¸ªèŠ±è‰²ä¸­æŠ½å–ä¸¤å¼ éšæœºç‰Œ

```{python}
def get_suit(card):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè·å–ç‰Œçš„èŠ±è‰²
    return card[-1]

deck.groupby(get_suit).apply(draw, n=2)  # æŒ‰èŠ±è‰²åˆ†ç»„ï¼Œä»æ¯ä¸ªèŠ±è‰²ä¸­æŠ½å– 2 å¼ ç‰Œ
```

# ç¤ºä¾‹ï¼šç»„åŠ æƒå¹³å‡å’Œç›¸å…³æ€§

## ç»„åŠ æƒå¹³å‡

```{python}
df = pd.DataFrame({
    "category": ["a", "a", "a", "a",  # åˆ›å»ºä¸€ä¸ª DataFrame
                 "b", "b", "b", "b"],
    "data": np.random.standard_normal(8),
    "weights": np.random.uniform(size=8)
})

grouped = df.groupby("category")  # æŒ‰ category åˆ†ç»„

def get_wavg(group):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—ç»„çš„åŠ æƒå¹³å‡å€¼
    return np.average(group["data"], weights=group["weights"])

grouped.apply(get_wavg)  # åº”ç”¨ get_wavg å‡½æ•°
```

## ä¸ SPXï¼ˆæ ‡å‡†æ™®å°” 500 æŒ‡æ•°ï¼‰çš„ç›¸å…³æ€§

```{python}
# å‡è®¾ examples æ–‡ä»¶å¤¹å­˜åœ¨ä¸”åŒ…å« stock_px.csv æ–‡ä»¶
close_px = pd.read_csv("examples/stock_px.csv", parse_dates=True, index_col=0)  # ä» CSV æ–‡ä»¶åŠ è½½æ•°æ®

rets = close_px.pct_change().dropna()  # è®¡ç®—æ”¶ç›Šç‡å¹¶åˆ é™¤ç¼ºå¤±å€¼

def get_year(x):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè·å–å¹´ä»½
    return x.year

by_year = rets.groupby(get_year)  # æŒ‰å¹´ä»½åˆ†ç»„

def spx_corr(group):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—ä¸ SPX çš„ç›¸å…³æ€§
    return group.corrwith(group["SPX"])

by_year.apply(spx_corr)  # åº”ç”¨ spx_corr å‡½æ•°
```

## åˆ—é—´ç›¸å…³æ€§ï¼ˆApple å’Œ Microsoftï¼‰

```{python}
def corr_aapl_msft(group):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®— Apple å’Œ Microsoft ä¹‹é—´çš„ç›¸å…³æ€§
    return group["AAPL"].corr(group["MSFT"])

by_year.apply(corr_aapl_msft)  # åº”ç”¨ corr_aapl_msft å‡½æ•°
```

# ç¤ºä¾‹ï¼šç»„çº¿æ€§å›å½’

```{python}
import statsmodels.api as sm  # å¯¼å…¥ statsmodels åº“

def regress(data, yvar=None, xvars=None):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ‰§è¡Œçº¿æ€§å›å½’
    Y = data[yvar]
    X = data[xvars]
    X["intercept"] = 1.
    result = sm.OLS(Y, X).fit()
    return result.params

by_year.apply(regress, yvar="AAPL", xvars=["SPX"])  # åº”ç”¨ regress å‡½æ•°
```

# `transform`ï¼šç»„è½¬æ¢å’Œâ€œå±•å¼€çš„â€ GroupBy

## `transform` æ–¹æ³•

`transform` ç±»ä¼¼äº `apply`ï¼Œä½†ï¼š

-   å¯ä»¥ç”Ÿæˆè¦å¹¿æ’­çš„æ ‡é‡ã€‚
-   ç”Ÿæˆä¸è¾“å…¥å½¢çŠ¶ç›¸åŒçš„å¯¹è±¡ã€‚
-   ä¸å¾—æ”¹å˜å…¶è¾“å…¥ã€‚

```{python}
df = pd.DataFrame({'key': ['a', 'b', 'c'] * 4,
                   'value': np.arange(12.)})
g = df.groupby('key')['value']

def get_mean(group):  #å®šä¹‰ä¸€ä¸ªå‡½æ•°æ±‚å¹³å‡å€¼
    return group.mean()
g.transform(get_mean)
```

æˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨ GroupBy `agg` æ–¹æ³•ä¸€æ ·ä¼ é€’å­—ç¬¦ä¸²åˆ«åï¼š

```{python}
g.transform('mean')
```

ä¸ `apply` ç±»ä¼¼ï¼Œ`transform` é€‚ç”¨äºè¿”å› Series çš„å‡½æ•°ï¼Œä½†ç»“æœå¿…é¡»ä¸è¾“å…¥å¤§å°ç›¸åŒã€‚ ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¾…åŠ©å‡½æ•°å°†æ¯ä¸ªç»„ä¹˜ä»¥ 2ï¼š

```{python}
def times_two(group):
      return group * 2
g.transform(times_two)
```
ä½œä¸ºä¸€ä¸ªæ›´å¤æ‚çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªç»„çš„é™åºæ’åï¼š

```{python}
def get_ranks(group):
    return group.rank(ascending=False)
g.transform(get_ranks)
```

## â€œå±•å¼€çš„â€åˆ†ç»„æ“ä½œ

â€œå±•å¼€çš„â€æ“ä½œé€šå¸¸æ¯” `apply` å¿«ï¼š

```{python}
def normalize(x):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿›è¡Œæ ‡å‡†åŒ–
    return (x - x.mean()) / x.std()

g.transform(normalize)  # ä½¿ç”¨ transform è¿›è¡Œæ ‡å‡†åŒ–
```

```{python}
g.apply(normalize)  # ä½¿ç”¨ apply è¿›è¡Œæ ‡å‡†åŒ–
```

```{python}
normalized = (df['value'] - g.transform('mean')) / g.transform('std')  # å±•å¼€çš„åˆ†ç»„æ“ä½œ
normalized
```

# æ•°æ®é€è§†è¡¨å’Œäº¤å‰è¡¨

## ä»€ä¹ˆæ˜¯æ•°æ®é€è§†è¡¨ï¼ŸğŸ¤”

æŒ‰é”®èšåˆæ•°æ®ï¼Œå°†å…¶æ’åˆ—æˆçŸ©å½¢ã€‚åœ¨ç”µå­è¡¨æ ¼ä¸­å¾ˆå¸¸è§ã€‚

## ä½¿ç”¨ pandas åˆ›å»ºæ•°æ®é€è§†è¡¨

`pivot_table` åˆ©ç”¨ `groupby` å’Œåˆ†å±‚ç´¢å¼•ï¼š

```{python}
tips.pivot_table(
    values=['total_bill', 'tip'],  # è¦èšåˆçš„åˆ—
    index=['day', 'smoker'],  # è¡Œç´¢å¼•
    aggfunc='mean'  # èšåˆå‡½æ•°
)
```

## æŒ‰å¤šä¸ªå˜é‡åˆ†ç»„

```{python}
tips.pivot_table(index=["time", "day"], columns="smoker",  # è¡Œå’Œåˆ—
                 values=["tip_pct", "size"])  # è¦èšåˆçš„åˆ—
```

## æ·»åŠ è¾¹è·ï¼ˆéƒ¨åˆ†æ€»è®¡ï¼‰

```{python}
tips.pivot_table(index=["time", "day"], columns="smoker",
                 values=["tip_pct", "size"], margins=True)  # æ·»åŠ è¾¹è·
```

## ä½¿ç”¨ä¸åŒçš„èšåˆå‡½æ•°

```{python}
tips.pivot_table(index=["time", "smoker"], columns="day",
                 values="tip_pct", aggfunc=len, margins=True)  # ä½¿ç”¨ len ä½œä¸ºèšåˆå‡½æ•°
```

## äº¤å‰è¡¨ (Crosstab)

Crosstab è®¡ç®—ç»„é¢‘ç‡ï¼š

```{python}
from io import StringIO

data = pd.read_table(StringIO("""Sample Nationality Handedness
1 USA Right-handed
2 Japan Left-handed
3 USA Right-handed
4 Japan Right-handed
5 Japan Left-handed
6 Japan Right-handed
7 USA Right-handed
8 USA Left-handed
9 Japan Right-handed
10 USA Right-handed"""), sep="\s+")

pd.crosstab(data["Nationality"], data["Handedness"], margins=True)  # åˆ›å»ºäº¤å‰è¡¨
```

```{python}
pd.crosstab([tips["time"], tips["day"]], tips["smoker"], margins=True)  # åˆ›å»ºäº¤å‰è¡¨
```

# æ€»ç»“

-   æˆ‘ä»¬æ¢ç´¢äº†æ•°æ®èšåˆã€`groupby`ã€`apply`ã€`transform`ã€‚
-   æˆ‘ä»¬åˆ›å»ºäº†æ•°æ®é€è§†è¡¨å’Œäº¤å‰è¡¨ã€‚
-   å¯¹äºæ€»ç»“ã€æ¯”è¾ƒå’Œå‡†å¤‡æ•°æ®è‡³å…³é‡è¦ã€‚

# æ€è€ƒä¸è®¨è®º

-   åˆ†ç»„æ“ä½œçš„å…¶ä»–å®é™…åœºæ™¯ï¼Ÿ
-   è¿™äº›æŠ€æœ¯çš„åˆ›é€ æ€§ç»„åˆï¼Ÿ
-   `groupby` çš„å±€é™æ€§ï¼Ÿ
-   `apply`ã€`transform` å’Œç›´æ¥èšåˆä¹‹é—´çš„æƒè¡¡ï¼Ÿ
-   â€œå±•å¼€çš„â€ä¸æ€§èƒ½æœ‰ä½•å…³ç³»ï¼Ÿ
